Load and Display Images:
    I am adopting these functions from project 1:
        I tested how to display and load images using the open cv and PIL packages (can be seen in my comments). Both are simple to use and I noticed that I can use less lines of code when using PIL, however, I opted to go with open cv because cv2 is more common from what I've heard and I wanted to understand it better.

        As I kept progressing through the assignment I also noticed that using open cv was a lot more convenient for implementing the other functions in the assignment and also getting to display the image after doing some sort of filtering or transformation.

Object Recognition:
    generate_vocabulary(train_data_file):
        I first loaded and parsed the list of training images provided. Each image's file path was extracted and processed. Then, for each image, I used the Scale-Invariant Feature Transform (SIFT) algorithm to extract key features. With SIFT, I detected key points and computed descriptors representing local features. These descriptors were accumulated from all the images into a comprehensive list. Next, I applied K-means clustering with 100 clusters to these descriptors. This clustering process allowed me to group similar features together, effectively creating visual "words." The cluster centers obtained from K-means served as the vocabulary set, representing the characteristic features of the dataset. Finally, I returned this vocabulary set.

    extract_features(image, vocabulary):
        I utilized the provided image along with the visual vocabulary previously generated. First, I employed the SIFT algorithm to detect key points and compute descriptors representing local features within the image. These descriptors were then compared to the visual vocabulary, with each descriptor matched to the closest visual word based on Euclidean distance. Using this matching, I constructed a Bag-of-Words (BOW) count vector, initialized with zeros and having a length equal to the vocabulary size. For each descriptor, I found the nearest visual word and incremented the corresponding bin in the BOW vector. Finally, I returned this BOW vector, which encapsulates the frequency of occurrence of each visual word in the image.

    train_classifier(train_data_file, vocab):
        I began by reading and parsing the training data file, extracting file paths along with their corresponding labels. For each image in the training dataset, I loaded it using the load_img function and then extracted features from the image using the previously defined function extract_features, passing the image and the visual vocabulary. These features were collected alongside their corresponding labels. After accumulating the features and labels, I initialized a Support Vector Machine (SVM) classifier using scikit-learn's svm.SVC() function. Subsequently, I trained the classifier by fitting it to the extracted features and labels using the fit() method. Finally, I returned the trained classifier.

    classify_image(classifier, test_image, vocab):
        I utilized the trained classifier along with the test image and the visual vocabulary. First, I extracted features from the test image using the provided extract_features function, passing the image and the visual vocabulary as parameters. These features were then used to generate a feature vector representing the test image. Next, I employed the trained classifier to predict the label of the test image based on its feature vector. The prediction was obtained using the predict method of the classifier, passing the feature vector as input. Finally, I returned the output classification, which represents the label predicted by the classifier for the given test image.

Image Segmentation:
    threshold_image(image, low_thresh, high_thresh):
        I iterated through each pixel of the input image. For each pixel, if its intensity value was below the low threshold, I classified it as an "object" by setting its value to 0. If the intensity value was above the high threshold, I classified it as "background" by setting its value to 255. For pixels whose intensity values fell between the low and high thresholds, I performed additional checks to determine their classification based on neighboring pixels. I padded the image with zeros to prevent out-of-bounds access when checking neighboring pixels. Then, for each pixel, if any of its 8 neighbors had an intensity value above the high threshold, I classified the pixel as "background" by setting its value to 255. If none of the neighbors had an intensity value above the high threshold, I classified the pixel as an "object" by setting its value to 0. Finally, I returned the processed image.

    grow_regions(image):
        I began by creating an empty image to store the region map, initialized with zeros and having the same dimensions as the input image. Then, I established a label variable to assign a unique label to each region. Next, I iterated through each pixel in the input image. For each pixel, if it had not been assigned to any region (its corresponding value in the region map was 0), I initiated a region-growing process starting from that pixel. I used a stack data structure to keep track of pixels belonging to the current region. While the stack was not empty, I popped a pixel from the stack and assigned its corresponding position in the region map the current label, indicating it belongs to the current region. Then, I examined the neighboring pixels of the current pixel and added them to the stack if they met the specified criteria for region growing, based on the intensity similarity condition provided. This process continued until no more pixels could be added to the current region. Finally, I incremented the label to prepare for the next region and repeated the process until all pixels in the image were assigned to a region. The resulting region map was then returned.

    split_regions(image):
        I initialized an empty image to store the region map, which had the same dimensions as the input image. I set up a queue to store the regions that needed to be processed, starting with the entire image as the initial region. I then established a label variable to assign a unique label to each region. While there were regions in the queue, I dequeued a region and examined its properties. If the standard deviation of the pixel intensities within the region exceeded a certain threshold (indicating heterogeneity) and if the dimensions of the region were larger than a specified minimum size, I split the region into four quadrants and enqueued each quadrant for further processing. If the region was homogeneous, I assigned it the current label in the region map and incremented the label for the next region. This process continued until all regions in the queue were processed. The resulting region map was then returned as the output.

    merge_regions(image):
        I initialized a region map, where each pixel was initially assigned a unique label corresponding to its position in the image grid. Next, I iterated over each pixel in the image and examined its 4-connected neighborhood. For each neighbor, I checked if it belonged to a different region by comparing its label with the label of the current pixel. If the difference in intensity values between the current pixel and its neighbor was below a specified threshold, I merged the regions to which the pixels belonged by assigning the same label to all pixels in the neighbor's region as the current pixel's region. This process continued for all pixels in the image, ensuring that neighboring regions with similar intensity characteristics were merged together. Finally, I converted the region map to a uint8 data type and returned it as the output.

    segment_image(image):
        Initially, an empty dictionary named segmentation_maps is initialized to store the segmentation results. The function then applies thresholding to the input image, generating a binary segmentation map based on intensity thresholds. This map is labeled as "Thresholding" and added to the dictionary. Subsequently, region growing is employed to segment regions of similar intensity, creating a map labeled as "Region Growing." Lastly, region merging is applied to merge neighboring regions with similar intensity characteristics, resulting in a segmentation map labeled as "Region Merging." These segmentation maps are returned by the function. The labels are not returned because when the display_image is called, it only takes images, so only the images are returned by the functions.

Image Segmentation with K-Means:
    kmeans_segment(image):
        The input image is reshaped into a 2D array of pixels to prepare for clustering. Next, the criteria for convergence of the K-means algorithm are defined, specifying the maximum number of iterations and the desired accuracy. The optimal number of clusters, optimal_k, is determined to achieve effective segmentation, typically through experimentation or automated methods such as the elbow method. Then, K-means clustering is applied to the pixel data, with the specified number of clusters, using the OpenCV cv2.kmeans function. The resulting cluster centers are converted to 8-bit values, and the labels assigned to each pixel are flattened. Finally, the segmented image is reconstructed using the cluster centers and labels, and returned as the output of the function.