NOTE ABOUT PROVIDED TEST IMAGES:
    MONA LISA - test_img1.png and test_img2.png
        They both work for every function in p2, however, I noticed that there are some false positives that I was unable to fully resolve.

    GOLDEN GATE - test_img3.jpg and test_img4.jpg
        These images work for every function in p2, EXCEPT feature_matching where the detector is Moravec and the extrator is LBP only. I was unable to resolve this issue. The other functions run for everything else, also, when running feature_matching with Harris and HOG, it doesn't detect any matches. There were several false positives that I was unable to resolve as well.

Load and Display Images:
    I am adopting these functions from project 1:
        I tested how to display and load images using the open cv and PIL packages (can be seen in my comments). Both are simple to use and I noticed that I can use less lines of code when using PIL, however, I opted to go with open cv because cv2 is more common from what I've heard and I wanted to understand it better.

        As I kept progressing through the assignment I also noticed that using open cv was a lot more convenient for implementing the other functions in the assignment and also getting to display the image after doing some sort of filtering or transformation.

Moravec Detector:
    First, I initialized an empty list to store the keypoints and set a threshold value for corner detection. I padded the image with a border of size 2 to ensure that corners near the edges of the image can be properly detected using a 3x3 window. I iterate over each pixel in the padded image, excluding a border of 4 pixels from each edge to avoid out-of-bounds errors. For each pixel, a 3x3 window centered around it is extracted. The sum of squared differences (SSD) is computed between this window and its shifted versions in all eight directions. The SSD values are squared to emphasize larger differences and compared against a local maximum, sw. If the SSD value surpasses the threshold, indicating a significant change in intensity, the pixel is identified as a keypoint, and its coordinates (adjusted for the padding) are appended to the keypoints list. The list of detected keypoints in the original image is returned. Necessary adjustments were made to compensate for the padding added to the image.

LBP Features:
    First, a list of neighbor positions relative to the center is defined. An empty list to store the feature vector is initialized and the keypoint coordinates (x, y) are extracted. The image is padded with a border using the edge mode to ensure that LBP computation near the image boundary is consistent. A 16x16 window centered at the keypoint is then extracted from the padded image. The nested loop iterates over each pixel in the window. For each pixel, the function computes its LBP code by comparing the pixel intensity with its neighboring pixels according to the defined LBP neighborhood. The binary string representing the LBP code is constructed, where a '1' indicates a higher intensity than the center pixel, and a '0' indicates a lower intensity. The LBP code is converted to an integer and appended to the feature vector list. Once all LBP codes for the window are computed, a histogram with 256 bins is created to count the occurrence of each LBP pattern. The histogram is then normalized to obtain the final feature vector, which represents the distribution of LBP patterns around the keypoint. At the end, the normalized histogram is returned as a NumPy array, representing the LBP feature vector for the given keypoint in the image.

HOG Features:
    Before this function is called, the image is padded in the feature_matching function. Given the image and the coordinates of the keypoint, I calculated the gradients along the x and y axes using the numpy gradient function. I iterated over 16x16 pixel cells within a window centered around the keypoint, with each cell being subdivided into smaller cells of size 8x8. Within each cell, the magnitude and orientation of gradients for each pixel was calculated and a histogram of gradient orientations with 9 bins was built. These histograms were concatenated to form the feature vector for the window. The feature vector was normalized by dividing it into blocks of size 2x2 and normalizing each block independently. This vector is then returned.

Feature Matching:
    I first verify that the detector is either Moravec or Harris, and then I check if the descriptor is either LBP or HOG, otherwise a ValueError is raised. After that verification, the appropriate detector and descriptor functions are called to extract keypoints and features from the input images. One of the main differences, is that before the extract_HOG function is called, I pad the image just so that I don't get any out of bounds error. Following feature extraction, I iterate through the features of image1 and compares them with the features of image2 to find matches based on Euclidean distance. I store the indices of matching keypoints if the distance falls below a predefined threshold. Finally, a 2D array containing the matching keypoints for both images is returned, each row corresponds to a matched pair of keypoints between the images.

Harris Detector:
    I first start off by smoothing the input image using Gaussian blur to reduce noise. Then, I computed the horizontal and vertical derivatives using Sobel operators to calculate gradients in the image. These derivatives are then used to compute the products of derivatives, which are further convolved with a Gaussian window to obtain better results. Next, I calculated the corner response function R using the formula involving the determinant and trace of the gradient matrix. After computing R for each pixel, I then identified local maxima as potential keypoints. The threshold used is based on a fraction of the maximum R value and iterates through each pixel to check if it is a local maximum and exceeds the threshold. If a pixel satisfies these conditions, it is considered a keypoint, and its (x, y) coordinates are added to the list of keypoints. Finally, list of keypoints found in the image is returned.